import torch
import torch.nn as nn
import torch.optim as optim
import random
import numpy as np
from collections import deque
import matplotlib.pyplot as plt
from tr_simple_custom_taxi_env import SimpleTaxiEnv
from IPython.display import clear_output

# è¨­å®š device ç‚º GPU (è‹¥å¯ç”¨)
device = torch.device("cpu")
print(f"Using device: {device}")

class DQN(nn.Module):
    """è¼•é‡ç‰ˆ DQN"""
    def __init__(self, state_dim, action_dim):
        super(DQN, self).__init__()
        self.fc1 = nn.Linear(state_dim, 64)  # æ¸›å°‘ç¥ç¶“å…ƒæ•¸é‡
        self.fc2 = nn.Linear(64,  128)  # æ¸›å°‘ç¥ç¶“å…ƒæ•¸é‡    
        self.fc3 = nn.Linear(128, action_dim)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        return self.fc3(x)  # è¼¸å‡º 6 å€‹ Q-values

# ğŸ¯ è¨“ç·´è¶…åƒæ•¸ï¼ˆè¼•é‡ç‰ˆï¼‰
GAMMA = 0.99          # æŠ˜æ‰£å› å­
LR = 5e-4             # å­¸ç¿’ç‡ï¼ˆè¼ƒé«˜ï¼ŒåŠ é€Ÿæ”¶æ–‚ï¼‰
EPSILON_START = 1.0   # åˆå§‹æ¢ç´¢ç‡
EPSILON_END = 0.15    # æœ€å°æ¢ç´¢ç‡
EPSILON_DECAY = 0.9998 # æ¢ç´¢ç‡è¡°æ¸›
MEMORY_SIZE = 7500    # è¨˜æ†¶åº«å¤§å°ï¼ˆæ¸›å°‘ä½”ç”¨è¨˜æ†¶é«”ï¼‰
BATCH_SIZE = 32       # è¨“ç·´æ‰¹æ¬¡å¤§å°ï¼ˆæ¸›å°‘é¡¯å­˜éœ€æ±‚ï¼‰
TARGET_UPDATE = 10    # æ¯ 10 å€‹ episodes æ›´æ–°ç›®æ¨™ç¶²è·¯
EPISODES = 10000       # è¨“ç·´å›åˆæ•¸ï¼ˆæ¸›å°‘è¨“ç·´æ™‚é–“ï¼‰
MAX_STEPS_PER_EPISODE = 10000  # ğŸš¨ 10000 æ­¥å¾Œè‡ªå‹•çµæŸ
REPLAY_START = 500   # è¨˜æ†¶åº«æœ€å°‘è¦æœ‰ 500 æ¢è³‡æ–™æ‰èƒ½è¨“ç·´

# åˆå§‹åŒ–ç’°å¢ƒ
env = SimpleTaxiEnv()
state_dim = 16  # è§€å¯Ÿç‹€æ…‹ç‚º 16 ç¶­å‘é‡
action_dim = 6  

# å‰µå»º DQN & ç›®æ¨™ç¶²è·¯ä¸¦ç§»åˆ° GPU
policy_net = DQN(state_dim, action_dim).to(device)
target_net = DQN(state_dim, action_dim).to(device)
target_net.load_state_dict(policy_net.state_dict())
target_net.eval()

optimizer = optim.Adam(policy_net.parameters(), lr=LR)  
memory = deque(maxlen=MEMORY_SIZE)
epsilon = EPSILON_START

def store_experience(state, action, reward, next_state, done):
    memory.append((state, action, reward, next_state, done))

def select_action(state):
    global epsilon
    if random.random() < epsilon:
        return random.randint(0, action_dim - 1)
    else:
        with torch.no_grad():
            # å°‡ state ç§»åˆ° GPU
            state_tensor = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)
            return torch.argmax(policy_net(state_tensor)).item()

def train():
    if len(memory) < REPLAY_START:
        return

    batch = random.sample(memory, BATCH_SIZE)
    states, actions, rewards, next_states, dones = zip(*batch)

    # ç¢ºä¿æ‰€æœ‰ tensor éƒ½å»ºç«‹åœ¨ GPU ä¸Š
    states = torch.tensor(states, dtype=torch.float32, device=device)
    actions = torch.tensor(actions, dtype=torch.int64, device=device).unsqueeze(1)
    rewards = torch.tensor(rewards, dtype=torch.float32, device=device)
    next_states = torch.tensor(next_states, dtype=torch.float32, device=device)
    dones = torch.tensor(dones, dtype=torch.float32, device=device)

    q_values = policy_net(states).gather(1, actions).squeeze()
    next_q_values = target_net(next_states).max(1)[0].detach()
    target_q_values = rewards + GAMMA * next_q_values * (1 - dones)

    loss = nn.MSELoss()(q_values, target_q_values)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

# ğŸ® è¨“ç·´éç¨‹
reward_history = []

for episode in range(EPISODES):
    try:
        grid_size = random.randint(5, 10)  
        state, _ = env.reset(fixed_grid_size=grid_size)

        if episode == 0:
            print(f"âœ… ç¬¬ä¸€å›åˆé–‹å§‹ï¼Œåœ°åœ–å¤§å°: {grid_size}x{grid_size}")
            env.render_env()  

        total_reward = 0
        step_count = 0

        for step in range(MAX_STEPS_PER_EPISODE):
            action = select_action(state)
            next_state, reward, done, _ = env.step(action)
            store_experience(state, action, reward, next_state, done)

            state = next_state
            total_reward += reward
            step_count += 1

            if done:
                break

        if step_count >= MAX_STEPS_PER_EPISODE:
            done = True
            print(f"âš ï¸ Episode {episode} è¶…é {MAX_STEPS_PER_EPISODE} æ­¥ï¼Œè‡ªå‹•çµæŸ")

        train()  
        reward_history.append(total_reward)

        if episode % TARGET_UPDATE == 0:
            target_net.load_state_dict(policy_net.state_dict())

        epsilon = max(EPSILON_END, epsilon * EPSILON_DECAY)

        if episode % 100 == 0:
            print(f"ğŸ“Š Episode {episode}, Reward: {total_reward}, Grid Size: {grid_size}, Epsilon: {epsilon:.3f}")
            torch.save(policy_net.state_dict(), "dqn_taxi_light64.pth")
    except Exception as e:
        print(f"âŒ ç™¼ç”ŸéŒ¯èª¤: {e}")
        break  

# å„²å­˜ DQN æ¨¡å‹
torch.save(policy_net.state_dict(), "dqn_taxi_light64.pth")
print("DQN è¨“ç·´å®Œæˆï¼Œè¼•é‡åŒ–æ¨¡å‹å·²å„²å­˜ï¼")

# ğŸ“Š ç¹ªè£½çå‹µè¶¨å‹¢
plt.plot(reward_history)
plt.xlabel("Episodes")
plt.ylabel("Total Reward")
plt.title("DQN Lightweight Training Progress")
plt.show()
